{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bf2a0c",
   "metadata": {},
   "source": [
    "# Neural Network Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24393ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Import and read the csv.\n",
    "df = pd.read_csv(\"../ML_Data_&_Preprocessing/b3_df_nonEncoded.csv\", index_col=[0])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ad288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['leads'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951df0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on different state values for testing \n",
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[df['state']== 'MN']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(df2.columns[[0,6]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff396a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200565c8",
   "metadata": {},
   "source": [
    "# Split into train and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = df3['leads'].values\n",
    "X = df3.drop(['leads'], 1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1cbb6",
   "metadata": {},
   "source": [
    "# Compile, Train, Evaluate our Model 1 - NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05aec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "# using multi-layer perceptron (two layers)\n",
    "numInputFeatures = len(X_train[0])\n",
    "\n",
    "# I have X amount of columns\n",
    "# layer1 = input layer, typically equals number of input variables in data\n",
    "layer1 = 30\n",
    "# layer 2 = hidden layer, typically 2/3 of input layer\n",
    "layer2 = 15\n",
    "# layer 3 = hidden layer\n",
    "layer3= 2\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=layer1, \n",
    "                          input_dim=numInputFeatures, \n",
    "                          activation=\"hard_sigmoid\")\n",
    ")\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=layer2, \n",
    "                             activation=\"elu\"))\n",
    "\n",
    "# adding a third layer to increase accuracy \n",
    "nn.add(tf.keras.layers.Dense(units=layer3, \n",
    "                             activation='elu'))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"hard_sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48845696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    # checkpoint directory and file structure defined above\n",
    "    filepath=checkpoint_path,\n",
    "    # notified when checkpoint is being saved to the directory\n",
    "    verbose=1,\n",
    "    # checkpoint files take small space\n",
    "    save_weights_only=True,\n",
    "    # checkpoints saved every epoch\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a833b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# when brand 1, all states, accuracy = 36%\n",
    "# when brand 2, all states, accuracy = 36%\n",
    "# when brand 3, all states, accuracy = 36%\n",
    "\n",
    "# B1, CA = 78%\n",
    "# B2, CA = 80%\n",
    "# B3, CA = 77%\n",
    "\n",
    "# B3, PA = 100%\n",
    "# B3, TX = 76%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0f4c8",
   "metadata": {},
   "source": [
    "### B3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results_b3 = {\n",
    "        'State': ['PA', 'CA', 'TX', 'VA', 'NY', 'NC', 'SD', 'IL', 'DC', 'MA', 'TN', 'MN'],\n",
    "        'Predictive Accuracy': [1.0, .77, .76, .79, .84, .83, .95, .81, .84, .85, .75, .78],\n",
    "        'Tier': ['T1', 'T4', 'T4', 'T4', 'T3', 'T3', 'T1', 'T2', 'T1', 'T2', 'T3', 'T2']}\n",
    "data_results_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf73762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new df to display brand, state, and model accuracy \n",
    "df_results_b3 = pd.DataFrame(data_results_b3)\n",
    "df_results_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e91e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_b3.to_csv('b3_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the model to HDF5 file\n",
    "# nn.save(\"AlphabetSoupCharity_optimization.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
